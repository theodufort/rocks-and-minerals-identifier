{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found .env file at: c:\\Clones\\rocks-and-minerals-identifier\\.env\n",
            "✓ Loaded environment variables from .env file\n",
            "\n",
            "✓ HuggingFace token found in environment\n",
            "  Token length: 37 characters\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables from .env file\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ImportError:\n",
        "    print(\"⚠ python-dotenv not installed. Install it with: pip install python-dotenv\")\n",
        "    print(\"  Will use system environment variables only.\")\n",
        "    load_dotenv = None\n",
        "\n",
        "# Find the .env file in the project root\n",
        "# Start from current directory and go up to find .env file\n",
        "current_dir = Path().cwd()\n",
        "env_path = None\n",
        "\n",
        "# Try different locations: current dir, parent, parent.parent, etc.\n",
        "search_paths = [\n",
        "    current_dir / '.env',\n",
        "    current_dir.parent / '.env',\n",
        "    current_dir.parent.parent / '.env',\n",
        "    current_dir.parent.parent.parent / '.env',\n",
        "]\n",
        "\n",
        "for path in search_paths:\n",
        "    if path.exists():\n",
        "        env_path = path\n",
        "        break\n",
        "\n",
        "if env_path:\n",
        "    print(f\"Found .env file at: {env_path}\")\n",
        "    if load_dotenv:\n",
        "        load_dotenv(env_path)\n",
        "        print(\"✓ Loaded environment variables from .env file\")\n",
        "    else:\n",
        "        print(\"⚠ python-dotenv not available, cannot load .env file\")\n",
        "else:\n",
        "    print(\"⚠ .env file not found in search paths\")\n",
        "    print(\"  Searched in:\")\n",
        "    for path in search_paths:\n",
        "        print(f\"    - {path}\")\n",
        "    print(\"  Will use system environment variables only\")\n",
        "\n",
        "# Get HuggingFace token from environment\n",
        "HF_TOKEN = os.getenv('HF_TOKEN') or os.getenv('HUGGING_FACE_HUB_TOKEN') or os.getenv('HUGGINGFACE_TOKEN')\n",
        "\n",
        "if HF_TOKEN:\n",
        "    print(f\"\\n✓ HuggingFace token found in environment\")\n",
        "    print(f\"  Token length: {len(HF_TOKEN)} characters\")\n",
        "else:\n",
        "    print(\"\\n⚠ HuggingFace token not found in environment variables\")\n",
        "    print(\"  Please set one of the following in your .env file:\")\n",
        "    print(\"    - HF_TOKEN\")\n",
        "    print(\"    - HUGGING_FACE_HUB_TOKEN\")\n",
        "    print(\"    - HUGGINGFACE_TOKEN\")\n",
        "    print(\"\\n  Example .env file content:\")\n",
        "    print(\"    HF_TOKEN=your_token_here\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: c:\\Clones\\rocks-and-minerals-identifier\\data\\combined-datasets\n",
            "Loading dataset from: c:\\Clones\\rocks-and-minerals-identifier\\data\\combined-datasets\\train\n",
            "\n",
            "✓ Dataset loaded successfully\n",
            "  Splits: ['train']\n",
            "  Train size: 62088\n",
            "  Features: ['image', 'label']\n",
            "  Label type: <class 'datasets.features.features.Value'>\n",
            "\n",
            "  Total unique labels: 5246\n",
            "  Sample labels (first 10): ['abellaite', 'abelsonite', 'abenakiite-(ce)', 'abernathyite', 'abhurite', 'abramovite', 'abuite', 'acanthite', 'acetamide', 'achalaite']\n"
          ]
        }
      ],
      "source": [
        "# Load the combined dataset from disk\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the dataset (the dataset should be in the same directory as this notebook)\n",
        "# Try current directory first, then check parent directory\n",
        "current_dir = Path().cwd()\n",
        "dataset_path = current_dir / \"train\"\n",
        "\n",
        "# If train folder doesn't exist in current dir, try loading from current dir directly\n",
        "if not dataset_path.exists():\n",
        "    dataset_path = current_dir\n",
        "\n",
        "print(f\"Current directory: {current_dir}\")\n",
        "print(f\"Loading dataset from: {dataset_path}\")\n",
        "\n",
        "# Check if dataset_dict.json exists (indicates it's a DatasetDict)\n",
        "if (current_dir / \"dataset_dict.json\").exists():\n",
        "    ds = load_from_disk(str(current_dir))\n",
        "elif dataset_path.exists():\n",
        "    # Try loading from the train subdirectory\n",
        "    from datasets import DatasetDict, Dataset\n",
        "    train_ds = load_from_disk(str(current_dir / \"train\"))\n",
        "    ds = DatasetDict({\"train\": train_ds})\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Dataset not found in {current_dir}. Make sure you're running this notebook from the combined-datasets folder.\")\n",
        "\n",
        "print(f\"\\n✓ Dataset loaded successfully\")\n",
        "print(f\"  Splits: {list(ds.keys())}\")\n",
        "print(f\"  Train size: {len(ds['train'])}\")\n",
        "print(f\"  Features: {list(ds['train'].features.keys())}\")\n",
        "print(f\"  Label type: {type(ds['train'].features['label'])}\")\n",
        "\n",
        "# Show some statistics\n",
        "unique_labels = sorted(set(ds['train']['label']))\n",
        "print(f\"\\n  Total unique labels: {len(unique_labels)}\")\n",
        "print(f\"  Sample labels (first 10): {unique_labels[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repository ID: tedqc/mineral-dataset\n",
            "Private: True\n",
            "Max shard size: 200MB\n",
            "\n",
            "Dataset info:\n",
            "  Size: 62088 examples\n",
            "  Unique labels: 5246\n",
            "  Features: ['image', 'label']\n",
            "\n",
            "✓ Ready to upload to: tedqc/mineral-dataset\n"
          ]
        }
      ],
      "source": [
        "# Configure HuggingFace Hub upload settings\n",
        "# Set your HuggingFace repository ID here (format: username/dataset-name)\n",
        "# Example: \"myusername/combined-minerals-dataset\"\n",
        "dataset_repo_id = \"tedqc/mineral-dataset\"\n",
        "\n",
        "# Set to True to make the dataset private, False to make it public\n",
        "private = True\n",
        "\n",
        "# Maximum shard size (adjust if needed)\n",
        "max_shard_size = \"200MB\"\n",
        "\n",
        "print(f\"Repository ID: {dataset_repo_id}\")\n",
        "print(f\"Private: {private}\")\n",
        "print(f\"Max shard size: {max_shard_size}\")\n",
        "print(f\"\\nDataset info:\")\n",
        "print(f\"  Size: {len(ds['train'])} examples\")\n",
        "print(f\"  Unique labels: {len(unique_labels)}\")\n",
        "print(f\"  Features: {list(ds['train'].features.keys())}\")\n",
        "\n",
        "if dataset_repo_id == \"YOUR_USERNAME/YOUR_DATASET_NAME\":\n",
        "    print(\"\\n⚠ Please update 'dataset_repo_id' above with your HuggingFace username and dataset name\")\n",
        "else:\n",
        "    print(f\"\\n✓ Ready to upload to: {dataset_repo_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading dataset to HuggingFace Hub...\n",
            "This may take a while depending on dataset size...\n",
            "\n",
            "Repository: tedqc/mineral-dataset\n",
            "Private: True\n",
            "Max shard size: 200MB\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Uploading the dataset shards (num_proc=10): 100%|██████████| 58/58 [04:20<00:00,  4.49s/ shards]  \n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "✓ DATASET UPLOADED SUCCESSFULLY!\n",
            "==================================================\n",
            "\n",
            "Dataset URL: https://huggingface.co/datasets/tedqc/mineral-dataset\n",
            "Dataset size: 62088 examples\n",
            "Unique labels: 5246\n",
            "\n",
            "You can now use this dataset with:\n",
            "  from datasets import load_dataset\n",
            "  ds = load_dataset('tedqc/mineral-dataset')\n"
          ]
        }
      ],
      "source": [
        "# Upload the dataset to HuggingFace Hub\n",
        "if not HF_TOKEN:\n",
        "    print(\"⚠ Error: HuggingFace token not found!\")\n",
        "    print(\"Please set HF_TOKEN, HUGGING_FACE_HUB_TOKEN, or HUGGINGFACE_TOKEN in your .env file\")\n",
        "elif dataset_repo_id == \"YOUR_USERNAME/YOUR_DATASET_NAME\":\n",
        "    print(\"⚠ Please update 'dataset_repo_id' in the previous cell with your HuggingFace username and dataset name\")\n",
        "else:\n",
        "    print(\"Uploading dataset to HuggingFace Hub...\")\n",
        "    print(\"This may take a while depending on dataset size...\\n\")\n",
        "    print(f\"Repository: {dataset_repo_id}\")\n",
        "    print(f\"Private: {private}\")\n",
        "    print(f\"Max shard size: {max_shard_size}\\n\")\n",
        "    \n",
        "    try:\n",
        "        # Push to HuggingFace Hub\n",
        "        ds.push_to_hub(\n",
        "            repo_id=dataset_repo_id,\n",
        "            token=HF_TOKEN,\n",
        "            private=private,\n",
        "            max_shard_size=max_shard_size,\n",
        "            num_proc=10\n",
        "        )\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"✓ DATASET UPLOADED SUCCESSFULLY!\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"\\nDataset URL: https://huggingface.co/datasets/{dataset_repo_id}\")\n",
        "        print(f\"Dataset size: {len(ds['train'])} examples\")\n",
        "        print(f\"Unique labels: {len(unique_labels)}\")\n",
        "        print(\"\\nYou can now use this dataset with:\")\n",
        "        print(f\"  from datasets import load_dataset\")\n",
        "        print(f\"  ds = load_dataset('{dataset_repo_id}')\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error uploading dataset: {e}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"1. Check that your HuggingFace token is valid\")\n",
        "        print(\"2. Ensure you have write access to the repository\")\n",
        "        print(\"3. Check your internet connection\")\n",
        "        print(\"4. Verify the repository ID is correct (format: username/dataset-name)\")\n",
        "        print(\"5. Make sure the repository doesn't already exist (or you have permission to overwrite)\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
